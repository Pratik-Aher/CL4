{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be556d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/20, Keeper Reward: -131356.7453975291, Taker Reward: 500.0\n",
      "Episode 2/20, Keeper Reward: -1101.066171133041, Taker Reward: 500.0\n",
      "Episode 3/20, Keeper Reward: -621.2533612573226, Taker Reward: 500.0\n",
      "Episode 4/20, Keeper Reward: -1160.042802791231, Taker Reward: 500.0\n",
      "Episode 5/20, Keeper Reward: -4264.006279668631, Taker Reward: 500.0\n",
      "Episode 6/20, Keeper Reward: -612.1241716649247, Taker Reward: 500.0\n",
      "Episode 7/20, Keeper Reward: -1513.9227210023787, Taker Reward: 500.0\n",
      "Episode 8/20, Keeper Reward: -859.0678247214785, Taker Reward: 500.0\n",
      "Episode 9/20, Keeper Reward: -508.20899812200935, Taker Reward: 500.0\n",
      "Episode 10/20, Keeper Reward: -928.7351330086195, Taker Reward: 500.0\n",
      "Episode 11/20, Keeper Reward: -1024.6549173469894, Taker Reward: 500.0\n",
      "Episode 12/20, Keeper Reward: -1050.576208885849, Taker Reward: 500.0\n",
      "Episode 13/20, Keeper Reward: -814.0986366559483, Taker Reward: 500.0\n",
      "Episode 14/20, Keeper Reward: -1398.92991297358, Taker Reward: 500.27144614138393\n",
      "Episode 15/20, Keeper Reward: -1375.6878544648118, Taker Reward: 500.0\n",
      "Episode 16/20, Keeper Reward: -742.7959290514908, Taker Reward: 500.0\n",
      "Episode 17/20, Keeper Reward: -963.3460192921567, Taker Reward: 500.0\n",
      "Episode 18/20, Keeper Reward: -957.7142253183135, Taker Reward: 500.0\n",
      "Episode 19/20, Keeper Reward: -982.8955918224013, Taker Reward: 500.0\n",
      "Episode 20/20, Keeper Reward: -848.2467964273211, Taker Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class MultiAgentEnvironment:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 6  # Dimensionality of the state space (ball position + keeper position + taker position)\n",
    "        self.action_dim = 2  # Dimensionality of the action space (x, y displacement for each agent)\n",
    "        # Initialize agent positions\n",
    "        self.keeper_pos = np.array([0.0, 0.0])\n",
    "        self.taker_pos = np.array([random.uniform(-1, 1), random.uniform(-1, 1)])\n",
    "        self.ball_pos = np.array([random.uniform(-1, 1), random.uniform(-1, 1)])\n",
    "        self.max_steps = 1000  # Maximum number of steps before episode termination\n",
    "        self.current_step = 0  # Current step in the episode\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent positions and ball position\n",
    "        self.keeper_pos = np.array([0.0, 0.0])\n",
    "        self.taker_pos = np.array([random.uniform(-1, 1), random.uniform(-1, 1)])\n",
    "        self.ball_pos = np.array([random.uniform(-1, 1), random.uniform(-1, 1)])\n",
    "        self.current_step = 0\n",
    "        # Return initial state\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, keeper_action, taker_action):\n",
    "        # Update keeper position\n",
    "        self.keeper_pos += keeper_action\n",
    "        # Update taker position\n",
    "        self.taker_pos += taker_action\n",
    "        # Update ball position (towards the taker)\n",
    "        ball_move = self.taker_pos - self.ball_pos\n",
    "        self.ball_pos += ball_move / np.linalg.norm(ball_move)\n",
    "        # Increment step count\n",
    "        self.current_step += 1\n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= self.max_steps\n",
    "        # Calculate rewards\n",
    "        keeper_reward = -np.linalg.norm(self.keeper_pos - self.ball_pos)  # Negative distance to ball\n",
    "        taker_reward = np.linalg.norm(self.taker_pos - self.ball_pos)  # Positive distance to ball\n",
    "        # Return next state, rewards, and done flag\n",
    "        return self._get_state(), keeper_reward, taker_reward, done\n",
    "\n",
    "    def _get_state(self):\n",
    "        return np.concatenate([self.ball_pos, self.keeper_pos, self.taker_pos])\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IndependentQLearningAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=0.001, gamma=0.99):\n",
    "        self.q_network = QNetwork(input_dim, output_dim)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "            action = q_values.argmax().item()\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "        next_q_values = self.q_network(torch.tensor(next_state, dtype=torch.float32))\n",
    "        target = reward + self.gamma * next_q_values.max().item()\n",
    "        loss = self.loss_fn(q_values[action], torch.tensor(target, dtype=torch.float32))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = MultiAgentEnvironment()\n",
    "    keeper = IndependentQLearningAgent(env.state_dim, env.action_dim)\n",
    "    taker = IndependentQLearningAgent(env.state_dim, env.action_dim)\n",
    "    num_episodes = 20\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_keeper_reward = 0\n",
    "        total_taker_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            keeper_action = keeper.select_action(state)\n",
    "            taker_action = taker.select_action(state)\n",
    "            next_state, keeper_reward, taker_reward, done = env.step(keeper_action, taker_action)\n",
    "            keeper.update(state, keeper_action, keeper_reward, next_state)\n",
    "            taker.update(state, taker_action, taker_reward, next_state)\n",
    "            total_keeper_reward += keeper_reward\n",
    "            total_taker_reward += taker_reward\n",
    "            state = next_state\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Keeper Reward: {total_keeper_reward}, Taker Reward: {total_taker_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d2218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
